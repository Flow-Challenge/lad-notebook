{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to execute\n",
    "\n",
    "To execute the application, you need to have a valid flow token. You can obtain this token using one of the following methods:\n",
    "\n",
    "1. Generate a Flow Token Programmatically:\n",
    "    Run the `flow_token.py script`. If the script executes successfully, it will create a `.env` file containing your **FLOW_TENANT** and **FLOW_TOKEN**.\n",
    "\n",
    "\n",
    "\n",
    "2. Retrieve Your Flow Token from Browser Cookies:\n",
    "    Extract your flow token directly from your browser cookies. **Ensure you are logged in with your CI&T account**. For guidance, refer to this [YouTube tutorial](https://youtube.com/clip/Ugkxsv6UO3IpQ2ZLfbb61z4QN90XohMfzh22?si=hYZao7TG1E1SwvSK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "import threading\n",
    "import os\n",
    "import queue\n",
    "\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowToken = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model_engine = 'gemini-pro'\n",
    "def chatCompletion_v1(prompt, output_queue):\n",
    "\n",
    "    flow_llm_url = \"https://flow.ciandt.com/ai-orchestration-api/v1/openai/chat/completions\"\n",
    "    flow_tenant = os.getenv(\"FLOW_TENANT\") or \"lithiadw\"\n",
    "    flow_token = os.getenv(\"FLOW_TOKEN\") or flowToken\n",
    "\n",
    "    if not flow_token:\n",
    "        print(\"Flow token is missing. Please ensure FLOW_TOKEN environment variable is set.\")\n",
    "        return None\n",
    "\n",
    "    payload = json.dumps({\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": 4096,\n",
    "        \"temperature\": 0.7,\n",
    "        \"allowedModels\": [\n",
    "            \"gpt-4o-mini\"\n",
    "        ],\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "    })\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Accept': '*/*',\n",
    "        'FlowTenant': flow_tenant,\n",
    "        'FlowAgent': \"Kanjih-Tools\",\n",
    "        'Authorization': f\"Bearer {flow_token}\",\n",
    "    }\n",
    "\n",
    "    print(f\"\"\"Requesting completion for: {prompt}\"\"\")\n",
    "\n",
    "    r =  requests.request(\"POST\", flow_llm_url, headers=headers, data=payload)\n",
    "    print(f\"Response: {r.status_code}\")\n",
    "    \n",
    "    if(r.status_code != 200):\n",
    "        print(f\"{r.text} please recreate your flow token\")\n",
    "        return None\n",
    "    \n",
    "    # Create a queue to collect results\n",
    "    result = {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": r.json()['choices'][0]['message']['content']\n",
    "    }\n",
    "    output_queue.put(result)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptsMessages = [\n",
    "    \"Please generate question for a interview of a Data Scientist\",\n",
    "    \"Please generate question for a interview of a Data Engineer\",\n",
    "    \"Please generate question for a interview of a Java Developer\",\n",
    "    \"Please generate question for a interview of a Kotlin Developer\",\n",
    "    \"Please generate question for a interview of a Kotlin Developer\",\n",
    "    \"Please generate question for a interview of a React Developer\",\n",
    "    \"Please generate question for a interview of a Architect\",\n",
    "    \"Please generate question for a interview of a UI|UX Designer\",\n",
    "]\n",
    "\n",
    "# Number of threads to create\n",
    "num_threads = len(promptsMessages)\n",
    "# Create a queue to collect results\n",
    "output_queue = queue.Queue()\n",
    "\n",
    "threads = []\n",
    "for i in range(num_threads):\n",
    "    # Create a thread, passing i+1 as the argument for print_numbers\n",
    "    thread = threading.Thread(target=chatCompletion_v1, args=(promptsMessages[i], output_queue))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "# Wait for all threads to complete\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "results = []\n",
    "# Get all results from the queue\n",
    "while not output_queue.empty():\n",
    "    result = output_queue.get()\n",
    "    results.append(result)\n",
    "\n",
    "print(f\"Results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lad-notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
